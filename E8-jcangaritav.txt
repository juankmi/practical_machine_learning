Dentro del campo de Machine Learning, se emplean métodos combinados o llamados métodos de ensamble, no siendo el único método existente, en otras palabras se debe tener en cuenta que una combinación de algoritmos de una determinada familia no es necesariamente una hipótesis de la misma, por lo que se pueden obtener mejores resultados que con los elementos individuales, pero también, por el mismo efecto, se puede correr el riesgo de sobreajustar el modelo. Dicho método, utiliza múltiples algoritmos de aprendizaje (como se mencionó anteriormente) para obtener un mejor rendimiento predictivo del que podría generarse empleando un único algoritmo de aprendizaje, es decir, la suma de las partes es mayor que cada parte individual.  Una de las desventajas de este tipo de métodos, es la elevación de los costos computacionales que produce el uso de varias hipótesis simultáneamente, sin embargo, se suelen emplear algoritmos rápidos como los arboles de decisión.  Dentro de los métodos de ensamble, el más conocido es el Bagging, que se trata de un meta algoritmo confeccionado para generar combinaciones de modelos a partir de una familia inicial, lo que genera una disminución de la varianza y por regla general evita por naturaleza el sobreajuste. Otro modelo empleado es el Boosting, que a diferencia del método anterior, trabaja sobre el total de la muestra, cambiando los pesos de los distintos datos con el fin de que las observaciones mal clasificadas adquieran mayor peso y puedan llegar a ser predichas en el siguiente ejercicio. Por último, se encuentra el método de subespacios aleatorios, el cual, se entrena con todo el conjunto de datos pero solo se considera un subconjunto de atributos.