En primera instancia, el Gradient Boosting Classifier es una técnica de machine learning para el análisis de clasificación (para este caso) en el cual, a partir de una serie de modelos débiles -típicamente arboles de decisión- produce un modelo predictivo más robusto, que construye un modelo escalonado y lo generaliza optimizando así, la función de perdida. A diferencia de los algoritmos de Stochastic Gradient decent donde se mantiene fija la estructura del algoritmo, el gradient boosting no la da como fija, y lo contrario trata de encontrar la mejor solución al parámetro P y la mejor función F. Por otro lado, el XGBoost (eXtrem Gradient Boosting) es una implementación de árboles de decisión con Gradient Boosting diseñada para minimizar la velocidad de ejecución y maximizar el rendimiento, lo que lo hace más eficiente que su antecesor, considerando la pérdida potencial de todas las segmentaciones posibles que posteriormente generaliza para crea una única segmentación con mayor capacidad predictiva.  El XGBoost toma únicamente valores numéricos como inputs, así que todos los datos que no tengan estas características deben ser transformados, y esta es una de las mayores “virtudes” de este algoritmo ya que esta característica es lo que la hace más eficiente, computacionalmente hablando. Otra de las bondades de algoritmo, es que tiene internamente algunos trucos que realizan regularización de la data, de tal manera que permite que se investiguen más rápidamente configuraciones de los hiperparametros