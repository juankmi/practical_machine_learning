Un árbol de decisión es un diagrama del conjunto de los posibles resultados de una serie de pasos, patrones o decisiones relacionados que permite comparar posibles pasos o acciones según lo que se este analizando: probabilidades, beneficios o costos (por ejemplo). Dentro de las ventajas que podemos resaltar del uso de arboles es que son muy fáciles en entender, no requieren preparación exhaustiva de la data, se combinan fácilmente, entre otras.  En data science, existen dos tipos generales de arboles de decisión: Árbol de regresión y árbol de clasificación. EL primero de ellos obedece a aquellos arboles cuyo resultado se puede considerar un numero real (variable continua) y el segundo cuando la variable dependiente o variable a explicar es de tipo cualitativa. Ahora bien, las tecnicas actuales, para obtener una mayor precisión en el modeloamiento, utilizan métodos de ensamblado, o agrupan distintos tipos de arboles, como por ejemplo Bagging, donde crea diferentes tipos de raboles haciendo remuestreo de los datos origen, el popular Random Forest que genera multiples arboles para aumentar la tasa de clasificación, arboles ampliados que pueden usar arboles d eregresion y de clasificación. Dentro de los algoritmos más utilizados para la generación de arboles, están: ID3, C4.5, ACR, CHAID (que personalmente uso bastante) y MARS.